# -*- coding: utf-8 -*-
"""M Usama_MSDS24045_01.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uEdZPZbeCXn8JYo64DofigyXLx2uI_uG
"""

from google.colab import drive
drive.mount('/content/drive')

# Loading Dataset
import pandas as pd
df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning Assignment/Assignment 01/Task-1 Dataset (California Housing)/california_housing_train.csv')
df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Deep Learning Assignment/Assignment 01/Task-1 Dataset (California Housing)/california_housing_test.csv')

# Splitting dataset into training and validation set
def split_dataset (df, split_ratio=0.85):
  n = len(df)
  train_size = int(n * split_ratio)
  df_train_set = df.iloc[:train_size]
  df_val_set = df.iloc[train_size:]
  return df_train_set, df_val_set

df_train_set, df_val_set = split_dataset(df_train)

import numpy as np

# Preprocessing Data

# Training Data
X_train = df_train_set.drop('target', axis=1).values
y_train = df_train_set['target'].values.reshape(-1,1)      #converting target column into 2D array

# Validation Data
X_val = df_val_set.drop('target', axis=1).values
y_val = df_val_set['target'].values.reshape(-1,1)

# Test Dataset
X_test = df_test.drop('target', axis=1).values
y_test = df_test['target'].values.reshape(-1,1)

# Computing mean and standard deviation for training data
mean = np.mean(X_train, axis=0)
std = np.std(X_train, axis=0)

# # Normailizing
# def normalize(X, std, mean):
#   return (X - mean) / std

def normalize(X, mean, std):
    return (X - mean) / (std + 1e-8)  # Add small epsilon to avoid division by zero


X_train_norm = normalize(X_train, std, mean)
X_val_norm = normalize(X_val, std, mean)
X_test_norm = normalize(X_test, std, mean)

# Data Loader

from torch.utils.data import Dataset, DataLoader

class HousingDataset(Dataset):
  def __init__(self, X, y):
    self.X = X
    self.y = y

  def __len__(self):
    return len(self.X)

  def __getitem__(self, idx):
    return self.X[idx], self.y[idx]


batch_size = 32
train_dataset = HousingDataset(X_train_norm, y_train)
val_dataset = HousingDataset(X_val_norm, y_val)

train_laoder = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)

# Initializing Model Parameter

def initialize_weights(dim):
    return np.random.randn(dim, 1) * np.sqrt(2.0 / dim)  # He initialization


# Input dimensions
input_dim = X_train_norm.shape[1]
theta = initialize_weights(input_dim)

# Training Loop
def train(theta, train_loader, val_loader, n_epochs, learning_rate):
  train_losses, val_losses = [] , []

  for epoch in range(n_epochs):
    theta, train_loss = sgd_step(theta, train_loader, learning_rate)
    val_loss = compute_loss(theta, val_loader)

    train_losses.append(train_loss)
    val_losses.append(val_loss)

    print(f'Epoch {epoch + 1}: Train Loss = {train_loss: .4f}, Val Loss = {val_loss: .4f}')

  return theta, train_losses, val_losses


def sgd_step(theta, train_loader, learning_rate):
  losses = []

  for X_batch, y_batch in train_loader:
    X_batch = X_batch.numpy().T
    y_batch = y_batch.numpy()

    # Forward Pass
    y_hat = X_batch.T @ theta          # X.T * W ( batch_size, 1)

    # Loss
    loss = np.mean((y_hat - y_batch) ** 2) / 2
    losses.append (loss)

    # Gradient
    error = y_hat - y_batch
    # gradient = X_batch @ error
    gradient = (X_batch @ error) / len(y_batch)

    # update theta
    theta = theta - learning_rate * gradient

  return theta, np.mean(losses)


def compute_loss(theta, dataloader):
  losses = []

  for X_batch, y_batch in dataloader:
    X_batch = X_batch.numpy().T
    y_batch = y_batch.numpy()

    y_hat = X_batch.T @ theta

    loss = np.mean((y_hat - y_batch) ** 2) / 2
    losses.append(loss)

  return np.mean(losses)


# Hyperparameters
n_epochs = 100
learning_rate = 0.001



# Train the model
theta, train_losses, val_losses = train(theta, train_laoder, val_loader, n_epochs, learning_rate)

# Save the trained model

import pickle

model = {
    'theta' : theta,
    'mean' : mean,
    'std' : std
}

with open('model.pkl' , 'wb') as f:
  pickle.dump(model, f)

print(f"X_test contains NaN: {np.isnan(X_test).any()}")
print(f"X_test contains inf: {np.isinf(X_test).any()}")
print(f"y_test contains NaN: {np.isnan(y_test).any()}")
print(f"y_test contains inf: {np.isinf(y_test).any()}")
print(f"Mean contains NaN: {np.isnan(mean).any()}")
print(f"Mean contains inf: {np.isinf(mean).any()}")
print(f"Std contains NaN: {np.isnan(std).any()}")
print(f"Std contains inf: {np.isinf(std).any()}")
print(f"Theta contains NaN: {np.isnan(theta).any()}")
print(f"Theta contains inf: {np.isinf(theta).any()}")

# Evaluating the model on Testing Data

with open('model.pkl', 'rb') as f:
  model = pickle.load(f)

theta = model['theta']
mean = model['mean']
std = model['std']

# Predicting on test data
test_X_norm = normalize(X_test_norm, mean, std)
y_hat_test = test_X_norm @ theta

# Computing Test Loss
test_loss = np.mean((y_hat_test - y_test) ** 2) / 2
print(f'Test Loss: {test_loss:.4f}')

# R_2 Score
ss_total = np.sum((y_test - np.mean(y_test)) ** 2)
ss_residual = np.sum((y_test - y_hat_test) ** 2)

r2_score = 1 - (ss_residual / ss_total)
print(f'R2 Score : {r2_score:.4f}')